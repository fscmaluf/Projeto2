{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto 2 - Classificador Autom√°tico de Sentimento\n",
    "\n",
    "Voc√™ foi contratado por uma empresa parar analisar como os clientes est√£o reagindo a um determinado produto no Twitter. A empresa deseja que voc√™ crie um programa que ir√° analisar as mensagens dispon√≠veis e classificar√° como \"relevante\" ou \"irrelevante\". Com isso ela deseja que mensagens negativas, que denigrem o nome do produto, ou que mere√ßam destaque, disparem um foco de aten√ß√£o da √°rea de marketing.<br /><br />\n",
    "Como aluno de Ci√™ncia dos Dados, voc√™ lembrou do Teorema de Bayes, mais especificamente do Classificador Naive-Bayes, que √© largamente utilizado em filtros anti-spam de e-mails. O classificador permite calcular qual a probabilidade de uma mensagem ser relevante dadas as palavras em seu conte√∫do.<br /><br />\n",
    "Para realizar o MVP (*minimum viable product*) do projeto, voc√™ precisa implementar uma vers√£o do classificador que \"aprende\" o que √© relevante com uma base de treinamento e compara a performance dos resultados com uma base de testes.<br /><br />\n",
    "Ap√≥s validado, o seu prot√≥tipo poder√° tamb√©m capturar e classificar automaticamente as mensagens da plataforma.\n",
    "\n",
    "## Informa√ß√µes do Projeto\n",
    "\n",
    "Prazo: 19/Set at√© √†s 23:59.<br />\n",
    "Grupo: 2 ou 3 pessoas - grupos com 3 pessoas ter√° uma rubrica diferenciada.<br /><br />\n",
    "Entreg√°veis via GitHub: \n",
    "* Arquivo notebook com o c√≥digo do classificador, seguindo as orienta√ß√µes abaixo.\n",
    "* Arquivo Excel com as bases de treinamento e teste totalmente classificado.\n",
    "\n",
    "**N√ÉO gravar a key do professor no arquivo**\n",
    "\n",
    "\n",
    "### Entrega Intermedi√°ria: Check 1 - APS 2\n",
    "\n",
    "At√© o dia 10/Set √†s 23:59, xlsx deve estar no Github com as seguintes evid√™ncias: \n",
    "\n",
    "  * Produto escolhido.\n",
    "  * Arquivo Excel contendo a base de treinamento e a base de testes j√° classificadas.\n",
    "\n",
    "Sugest√£o de leitura:<br />\n",
    "https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## Parte I - Adquirindo a Base de Dados\n",
    "\n",
    "Acessar o notebook **Projeto-2-Planilha** para realizar a coleta dos dados. O grupo deve classificar os dados coletados manualmente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Parte II - Montando o Classificador Naive-Bayes\n",
    "\n",
    "Com a base de treinamento montada, comece a desenvolver o classificador. N√£o se esque√ßa de implementar o Laplace Smoothing (https://en.wikipedia.org/wiki/Laplace_smoothing).\n",
    "\n",
    "Opcionalmente: \n",
    "* Limpar as mensagens removendo os caracteres: enter, :, \", ', (, ), etc. N√£o remover emojis.<br />\n",
    "* Corrigir separa√ß√£o de espa√ßos entre palavras e/ou emojis.\n",
    "* Propor outras limpezas/transforma√ß√µes que n√£o afetem a qualidade da informa√ß√£o.\n",
    "\n",
    "Escreva o seu c√≥digo abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from emoji import UNICODE_EMOJI\n",
    "\n",
    "df = pd.read_excel('tweets_netflix_treinamento.xlsx')\n",
    "\n",
    "df.replace(regex=['\\n', '\\t'], value = '', inplace = True) #corrigindo separa√ß√£o entre enter e tab - pt.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = []\n",
    "\n",
    "remover = ['?', '!', ',', ';', '.', '‚Äù', '‚Äú', ')',\n",
    "           '(', '*', '...', ':', '‚Ä¶', '{', '}', '\"',\n",
    "           '\"', '=', \"'\", '‚Ä¢', '|', '-', '/'] #caracteres que devem ser remov√≠dos\n",
    "for i in range(len(df['Treinamento'])):\n",
    "    m = ''\n",
    "    tweet = df['Treinamento'][i]\n",
    "    for letra in tweet:\n",
    "        if letra in UNICODE_EMOJI:\n",
    "            m += ' ' + letra  + ' ' #corrigindo espa√ßamento entre caracteres - pt.2\n",
    "        elif letra not in remover:\n",
    "            m += letra\n",
    "    tweets.append(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_final = []\n",
    "words = ['rt', 'a', 'is', 'me', 'in', 'netflix', 'as', 'it',\n",
    "         'we', 'us', 'of', 'i', 'to', 'the', 'by', 'im', 'so',\n",
    "         'are', 'or', 'has', 'have', 'their', 'that', 'else',\n",
    "         'on', 'be', 'his', 'our', 'you', 'my', 'its', 'for',\n",
    "         \"it‚Äôs\", 'this', 'if']\n",
    "for i in tweets:\n",
    "    post = i.split()\n",
    "    palavras = ''\n",
    "    for e in post:\n",
    "        #removendo links, hashtags, @, e demais termos/palavras in√∫teis\n",
    "        if e[0:4] != 'http' and e[0:1] != '@' and e[:1] != '#' and e not in words:\n",
    "            palavras += '' + e + ' '\n",
    "        elif e in UNICODE_EMOJI:\n",
    "            palavras += '' + e + ''\n",
    "            \n",
    "    tweets_final.append(palavras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(tweets_final, columns=['Treinamento'])\n",
    "data = data.join(df['Relev√¢ncia'], how='inner')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tudo = []\n",
    "\n",
    "for tweet in data['Treinamento']:\n",
    "    a = tweet.split(' ')\n",
    "    for palavra in a:\n",
    "        if palavra not in tudo:\n",
    "            tudo.append(palavra)\n",
    "            \n",
    "conta_relevante = 0\n",
    "conta_irrelevante = 0\n",
    "\n",
    "for i in range(len(data['Treinamento'])):\n",
    "    a = data[\"Treinamento\"][i].split(\" \")\n",
    "    if data['Relev√¢ncia'][i] == 1:\n",
    "        for palavra in a:\n",
    "            conta_relevante += 1\n",
    "    else:\n",
    "        for palavra in a:\n",
    "            conta_irrelevante += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevante = {}\n",
    "irrelevante = {}\n",
    "\n",
    "for palavra in tudo:\n",
    "    relevante[palavra] = 1\n",
    "    irrelevante[palavra] = 1\n",
    "    \n",
    "    \n",
    "for i in range(len(data['Treinamento'])):\n",
    "    a = data[\"Treinamento\"][i].split(\" \")\n",
    "    if data['Relev√¢ncia'][i] == 1:\n",
    "        for palavra in a:\n",
    "            relevante[palavra] += 1\n",
    "    else:\n",
    "        for palavra in a:\n",
    "            irrelevante[palavra] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_relevante = {}\n",
    "prob_irrelevante = {}\n",
    "\n",
    "for palavra in tudo:\n",
    "    prob_relevante[palavra] = relevante[palavra] / (conta_relevante + len(tudo)) \n",
    "    prob_irrelevante[palavra] = irrelevante[palavra] / (conta_irrelevante + len(tudo)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_rel = len(data[data[\"Relev√¢ncia\"] == 1])\n",
    "tweets_irrel = len(data[data[\"Relev√¢ncia\"] == 0])\n",
    "\n",
    "prob_tweets_irrel = tweets_irrel / (tweets_irrel + tweets_rel)\n",
    "prob_tweets_rel = tweets_rel / (tweets_irrel + tweets_rel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Verificando a performance\n",
    "\n",
    "Agora voc√™ deve testar o seu Classificador com a base de Testes.<br /><br /> \n",
    "\n",
    "Voc√™ deve extrair as seguintes medidas:\n",
    "* Porcentagem de positivos falsos (marcados como relevante mas n√£o s√£o relevantes)\n",
    "* Porcentagem de positivos verdadeiros (marcado como relevante e s√£o relevantes)\n",
    "* Porcentagem de negativos verdadeiros (marcado como n√£o relevante e n√£o s√£o relevantes)\n",
    "* Porcentagem de negativos falsos (marcado como n√£o relevante e s√£o relevantes)\n",
    "\n",
    "Obrigat√≥rio para grupos de 3 alunos:\n",
    "* Criar categorias intermedi√°rias de relev√¢ncia baseado na diferen√ßa de probabilidades. Exemplo: muito relevante, relevante, neutro, irrelevante e muito irrelevante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df2 = pd.read_excel('tweets_netflix_teste.xlsx')\n",
    "\n",
    "df2.replace(regex=['\\n', '\\t'], value = '', inplace = True) #corrigindo separa√ß√£o entre enter e tab - pt.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = []\n",
    "\n",
    "remover = ['?', '!', ',', ';', '.', '‚Äù', '‚Äú', ')',\n",
    "           '(', '*', '...', ':', '‚Ä¶', '{', '}', '\"',\n",
    "           '\"', '=', \"'\", '‚Ä¢', '|', '-', '/'] #caracteres que devem ser remov√≠dos\n",
    "for i in range(len(df2['Teste'])):\n",
    "    m = ''\n",
    "    tweet = df2['Teste'][i]\n",
    "    for letra in tweet:\n",
    "        if letra in UNICODE_EMOJI:\n",
    "            m += ' ' + letra  + ' ' #corrigindo espa√ßamento entre caracteres - pt.2\n",
    "        elif letra not in remover:\n",
    "            m += letra\n",
    "    tweets.append(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_final = []\n",
    "words = words = ['rt', 'a', 'is', 'me', 'in', 'netflix', 'as', 'it',\n",
    "         'we', 'us', 'of', 'i', 'to', 'the', 'by', 'im', 'so',\n",
    "         'are', 'or', 'has', 'have', 'their', 'that', 'else',\n",
    "         'on', 'be', 'his', 'our', 'you', 'my', 'its', 'for',\n",
    "         \"it‚Äôs\", 'this', 'if']\n",
    "for i in tweets:\n",
    "    post = i.split()\n",
    "    palavras = ''\n",
    "    for e in post:\n",
    "        #removendo links, hashtags, @, e demais termos/palavras in√∫teis\n",
    "        if e[0:4] != 'http' and e[0:1] != '@' and e[:1] != '#' and e not in words:\n",
    "            palavras += '' + e + ' '\n",
    "        elif e in UNICODE_EMOJI:\n",
    "            palavras += '' + e + ''\n",
    "            \n",
    "    tweets_final.append(palavras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.DataFrame(tweets_final, columns=['Teste'])\n",
    "data2 = data2.join(df2['Relev√¢ncia'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "previsao = []\n",
    "\n",
    "for tweet in data2['Teste']:\n",
    "    prob_rel = 1\n",
    "    prob_irrel = 1\n",
    "    palavras = tweet.split(' ')\n",
    "        \n",
    "    for palavra in palavras:\n",
    "        if palavra in prob_relevante:\n",
    "            prob_rel *= prob_relevante[palavra]\n",
    "        else:\n",
    "            prob_rel *= (1 / (conta_relevante + len(tudo)))\n",
    "            \n",
    "    for palavra in palavras:   \n",
    "        if palavra in prob_irrelevante:\n",
    "            prob_irrel *= prob_irrelevante[palavra]\n",
    "        else:\n",
    "            prob_irrel *= (1 / (conta_irrelevante + len(tudo)))\n",
    "            \n",
    "    prob_rel *= prob_tweets_rel\n",
    "    prob_irrel *= prob_tweets_irrel\n",
    "        \n",
    "    if prob_rel > prob_irrel:\n",
    "        previsao.append(1)\n",
    "    else:\n",
    "        previsao.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Teste</th>\n",
       "      <th>Relev√¢ncia</th>\n",
       "      <th>Previs√£o</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>week 3d film made with blender</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>got asked date and all want do stay bed and wa...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>they need put</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nearly headband still actually head strictly g...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mg aleatorio se lleva minecraft full accessopt...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>should keep true source material because what ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>just wanna spend day cuddled up w boo watching...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gonna catch up new luke cage season &amp;amp ozark...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>yeah just finished and still wrong 22 doing ba...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>black girls wanting swim but risking getting y...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>damn release new season series liked even clic...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>acquires worldwide rights genevieve nnajis mov...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>omw back room with snacks watch</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>stoned watching bed</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>am 24 year old woman who spending her saturday...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>buys nigerian comedy movie lionheart</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>shaku shaku included dance feature latest nba ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>better uk</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>sierra burgess loser</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>please</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>punch kick block repeat season 2 marvels now s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>vproud been part hugo blick‚Äôs frank and incred...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>mg aleatorio se lleva minecraft full accessopt...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>they need put</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>can hannah montana suite life zack and cody &amp;a...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>smdh am killer doesn‚Äôt feature any female murd...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>drunk much cant remember drunk anythingprinces...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>already finish season 6 wentworth before got üò©...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>want see sylvester again he sensible and frien...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>hate when shows do beyond frustrating</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>can‚Äôt even and chill your own home</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>3 jeremy corbyn proposed collecting digital li...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>very good show</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>they need put</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>probably youtube ive only cut out 4th time you...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>stoned watching bed</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>lionheart movie directed nollywood screen godd...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>sierra burgess loser</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>they need put</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>mg aleatorio se lleva minecraft full accessopt...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>when women paid less doing same work male peer...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>haven‚Äôt seen yet i‚Äôd recommend watch documenta...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>black girls wanting swim but risking getting y...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>acquired first movie from nigeria streaming gi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>team foxcatcher should watched every crimespor...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>black girls wanting swim but risking getting y...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>shaku shaku included dance feature latest nba ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>they need put</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>30 minutes from ‚ù§ Ô∏è bible belt but not same oz...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>was more livid with short duration though just...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>suck at and chill two minutes and i‚Äôm knocked ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>season 4 narcos november 16th</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>shit devastating not sure can make past eps 1 ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>think should keep good movies not take them of...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>leaked casting ciri</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>drunk much cant remember drunk anythingprinces...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>when whole world discussing about bold steps t...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>something new watch</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>mg aleatorio se lleva minecraft full accessopt...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>they finally put breakfast club and i‚Äôm crying</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Teste  Relev√¢ncia  Previs√£o\n",
       "0                      week 3d film made with blender            0         0\n",
       "1    got asked date and all want do stay bed and wa...           0         0\n",
       "2                                       they need put            0         0\n",
       "3    nearly headband still actually head strictly g...           0         1\n",
       "4    mg aleatorio se lleva minecraft full accessopt...           0         0\n",
       "5    should keep true source material because what ...           1         1\n",
       "6    just wanna spend day cuddled up w boo watching...           0         0\n",
       "7    gonna catch up new luke cage season &amp ozark...           0         0\n",
       "8    yeah just finished and still wrong 22 doing ba...           0         1\n",
       "9    black girls wanting swim but risking getting y...           0         0\n",
       "10   damn release new season series liked even clic...           1         0\n",
       "11   acquires worldwide rights genevieve nnajis mov...           0         0\n",
       "12                    omw back room with snacks watch            0         0\n",
       "13                                stoned watching bed            0         0\n",
       "14   am 24 year old woman who spending her saturday...           0         1\n",
       "15               buys nigerian comedy movie lionheart            0         0\n",
       "16   shaku shaku included dance feature latest nba ...           0         0\n",
       "17                                          better uk            0         0\n",
       "18                               sierra burgess loser            0         0\n",
       "19                                             please            0         0\n",
       "20   punch kick block repeat season 2 marvels now s...           0         0\n",
       "21   vproud been part hugo blick‚Äôs frank and incred...           0         1\n",
       "22   mg aleatorio se lleva minecraft full accessopt...           0         0\n",
       "23                                      they need put            0         0\n",
       "24   can hannah montana suite life zack and cody &a...           0         1\n",
       "25   smdh am killer doesn‚Äôt feature any female murd...           1         1\n",
       "26   drunk much cant remember drunk anythingprinces...           0         0\n",
       "27   already finish season 6 wentworth before got üò©...           0         0\n",
       "28   want see sylvester again he sensible and frien...           0         0\n",
       "29              hate when shows do beyond frustrating            0         0\n",
       "..                                                 ...         ...       ...\n",
       "170                can‚Äôt even and chill your own home            0         0\n",
       "171  3 jeremy corbyn proposed collecting digital li...           0         1\n",
       "172                                    very good show            0         0\n",
       "173                                     they need put            0         0\n",
       "174  probably youtube ive only cut out 4th time you...           0         0\n",
       "175                               stoned watching bed            0         0\n",
       "176  lionheart movie directed nollywood screen godd...           0         0\n",
       "177                              sierra burgess loser            0         0\n",
       "178                                     they need put            0         0\n",
       "179  mg aleatorio se lleva minecraft full accessopt...           0         0\n",
       "180  when women paid less doing same work male peer...           0         1\n",
       "181  haven‚Äôt seen yet i‚Äôd recommend watch documenta...           0         1\n",
       "182  black girls wanting swim but risking getting y...           0         0\n",
       "183  acquired first movie from nigeria streaming gi...           0         0\n",
       "184  team foxcatcher should watched every crimespor...           0         0\n",
       "185  black girls wanting swim but risking getting y...           0         0\n",
       "186  shaku shaku included dance feature latest nba ...           0         0\n",
       "187                                     they need put            0         0\n",
       "188  30 minutes from ‚ù§ Ô∏è bible belt but not same oz...           0         0\n",
       "189  was more livid with short duration though just...           0         0\n",
       "190  suck at and chill two minutes and i‚Äôm knocked ...           0         0\n",
       "191                     season 4 narcos november 16th            0         0\n",
       "192  shit devastating not sure can make past eps 1 ...           0         0\n",
       "193  think should keep good movies not take them of...           0         0\n",
       "194                               leaked casting ciri            0         1\n",
       "195  drunk much cant remember drunk anythingprinces...           0         0\n",
       "196  when whole world discussing about bold steps t...           0         1\n",
       "197                               something new watch            0         0\n",
       "198  mg aleatorio se lleva minecraft full accessopt...           0         0\n",
       "199    they finally put breakfast club and i‚Äôm crying            0         0\n",
       "\n",
       "[200 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2['Previs√£o'] = previsao\n",
    "data2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concluindo\n",
    "\n",
    "Escreva aqui a sua conclus√£o.<br /> \n",
    "Fa√ßa um comparativo qualitativo sobre as medidas obtidas.<br />\n",
    "Explique como s√£o tratadas as mensagens com dupla nega√ß√£o e sarcasmo.<br />\n",
    "Proponha um plano de expans√£o. Por que eles devem continuar financiando o seu projeto?<br />\n",
    "\n",
    "Opcionalmente: \n",
    "* Discorrer por que n√£o posso alimentar minha base de Treinamento automaticamente usando o pr√≥prio classificador, aplicado a novos tweets.\n",
    "* Propor diferentes cen√°rios de uso para o classificador Naive-Bayes. Cen√°rios sem intersec√ß√£o com este projeto.\n",
    "* Sugerir e explicar melhorias reais no classificador com indica√ß√µes concretas de como implementar (n√£o √© preciso codificar, mas indicar como fazer e material de pesquisa sobre o assunto).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If categorical variable has a category (in test data set), which was not observed in training data set, then model will assign a 0 (zero) probability and will be unable to make a prediction. This is often known as ‚ÄúZero Frequency‚Äù. To solve this, we can use the smoothing technique. One of the simplest smoothing techniques is called Laplace estimation.\n",
    "\n",
    "On the other side naive Bayes is also known as a bad estimator, so the probability outputs from predict_proba are not to be taken too seriously.\n",
    "\n",
    "Another limitation of Naive Bayes is the assumption of independent predictors. In real life, it is almost impossible that we get a set of predictors which are completely independent.\n",
    "\n",
    "**Melhorias**\n",
    "\n",
    "If continuous features do not have normal distribution, we should use transformation or different methods to convert it in normal distribution.\n",
    "If test data set has zero frequency issue, apply smoothing techniques ‚ÄúLaplace Correction‚Äù to predict the class of test data set.\n",
    "Remove correlated features, as the highly correlated features are voted twice in the model and it can lead to over inflating importance.\n",
    "Naive Bayes classifiers has limited options for parameter tuning like alpha=1 for smoothing, fit_prior=[True|False] to learn class prior probabilities or not and some other options (look at detail here). I would recommend to focus on your  pre-processing of data and the feature selection.\n",
    "You might think to apply some classifier combination technique like ensembling, bagging and boosting but these methods would not help. Actually, ‚Äúensembling, boosting, bagging‚Äù won‚Äôt help since their purpose is to reduce variance. Naive Bayes has no variance to minimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
